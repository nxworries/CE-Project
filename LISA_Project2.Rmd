---
title: "CE Project code"
author: "Chris Craig"
date: "3/10/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
test <- 1
```

```{r}
states <- read.csv("/Users/chriscraig/Downloads/states.csv")
survey.natcov <- read.csv("/Users/chriscraig/Downloads/survey_nativecovercat.csv")
states$state <- as.factor(states$state)
states$state_daren <- as.factor(states$state_daren)
states$state_condensed <- as.factor(states$state_condensed)
summary(states)
```

```{r}
# combine into 1 dataset, remove na values 
survey.trim <- survey.natcov[,1:3]
data <- merge(states, survey.trim, by = 'GlobalID')
data <- data[complete.cases(data), ]

```

```{r}
# some light data exploration
library(ggplot2)
ggplot(data = states) + 
  geom_bar(aes(x = state))
```

```{r}
ggplot(data = states) + 
  geom_bar(aes(x = state_daren))
```

```{r}
library(dplyr)
ggplot(data = states) + 
  geom_bar(aes(x = state_condensed))

```

```{r}
plyr::count(df = states, vars = 'state_condensed')
plyr::count(df = states, vars = 'native_indicators')
plyr::count(df = states, vars = c('state_condensed', 'native_indicators'))
```




```{r}
# sorting the data to help generate transition counts
data.sorted <- data[order(data$FK_transect_line,  data$survey_year),]
data.sorted$action <- data.sorted$V10
#head(data.sorted[,c(1, 2, 3, 4, 8, 9, 11)])
```

```{r}
# Generating random descisions with descisions = c(1, 2, 3) corresponding to 3 different actions, then adding to the sorted data
set.seed(321)
r.samp1 <- runif(length(data[,1]))
action1.prop <- .33
action2.prop <- .33
action3.prop <- .34
actions <- c()
for (i in 1:length(data[,1])){
  if (r.samp1[i] <= action1.prop){
    actions[i] <- 1
  }
  else if (r.samp1[i] > action1.prop & r.samp1[i] < (action1.prop + action2.prop)){
    actions[i] <- 2
    
  }
    else{
    actions[i] <- 3
  }
}

# add random actions to the dataset
data.sorted[,dim(data.sorted)[2]+1] <- actions
```




```{r}
# New state spaces for MC analysis, goal is to get rid of 0's in transition matrix, make sure its aperioic

data.sorted$state_condensed <- as.factor(data.sorted$state_condensed)
data.sorted$state.mc <- plyr::revalue(data.sorted$state_condensed, c('1'='1', '2'='1', '3'='1', '4' ='2', '5'='2', '6'='3', '7'='3', '8'='4', '9'='5', '10'='6', '11'='7', '12' = '8'))
data.sorted$state.mc <- as.double(data.sorted$state.mc)
data.sorted$state.simple <- plyr::revalue(data.sorted$state_condensed, c('1'='1', '2'='1', '3'='1', '4' ='1', '5'='1', '6'='1', '7'='2', '8'='2', '9'='2', '10'='3', '11'='3', '12' = '4'))
data.sorted$state.simple <- as.double(data.sorted$state.simple)
```

```{r}
# function to calculate transition counts, only useful with the sorted dataset, n = number of states, colname = name of column, pass as a string
transition.mat <- function(colname, n, dataset = data.sorted) {
  tran.counts <- matrix(data = 0, ncol = n, nrow = n)
  for (i in 1:(dim(dataset)[1]-1)){
  if ((dataset[i,]$FK_transect_line == dataset[i+1,]$FK_transect_line)){
    tran.counts[dataset[i,colname], dataset[i+1,colname]] = tran.counts[dataset[i,colname], dataset[i+1,colname]] + 1
  }}
  return(tran.counts)
}
```

```{r}
# Code to create bootstrapped probability transition matrices - DONT RUN - code needs to be cleaned 

# function to create resampled prob transition matrices, to be used to get bootstrapped prob transition matrices
resampled.TM <- function(col, n, dataset){
  b <- modelr::resample_bootstrap(dataset)
  b<- as.data.frame((b))
  b<- b[order(b$FK_transect_line,  b$survey_year),]
  transition.mat(col, n, b)
  }

bootsamples.condensed <- matrix(data = 0, nrow = 12, ncol =12)
bootsamples.mc <- matrix(data = 0, nrow = 8, ncol = 8)
bootsamples.simple <- matrix(data = 0, nrow = 4, ncol =4)
#for (i in 1:500){
#  bootsamples.condensed <- bootsamples.condensed + resampled.TM(7,12,data.sorted)
#  bootsamples.mc <- bootsamples.mc + resampled.TM(10,8,data.sorted)
#  bootsamples.simple <- bootsamples.simple + resampled.TM(11,4,data.sorted)
#}

#boot.prop.state.condensed <- prop.table(bootsamples.condensed, 1)
#boot.prop.mc <- prop.table(bootsamples.mc, 1)
#boot.prop.simple <- prop.table(bootsamples.condensed, 1)
```


```{r}
#  Probability transition matrices for each state space
trans.prop.state <- round(prop.table(transition.mat(colname = 'state', 20), 1), 3)
trans.prop.state.condensed <- round(prop.table(transition.mat(colname = 'state_condensed', 12), 1), 3)
trans.prop.state.mc <- round(prop.table(transition.mat(colname = 'state.mc', 8), 1), 3)
trans.prop.state.simple <- round(prop.table(transition.mat(colname = 'state.simple', 4), 1), 3)
```

```{r}
# Some nice plots
library(plot.matrix)
library(plotrix)

plot(trans.prop.state, digits=1, text.cell=list(cex=0.5), axis.col=list(side=1, las=1), axis.row = list(side=2, las=1), breaks = 9, col = RColorBrewer::brewer.pal(n = 9, name = "YlOrRd"), key = NULL, main = "Observed Transition Frequencies for Original State Space", xlab = 'state s\'', ylab = 'state s')
plot(trans.prop.state.condensed, digits=3, text.cell=list(cex=0.5), axis.col=list(side=1, las=1), axis.row = list(side=2, las=1), breaks = 9, col = RColorBrewer::brewer.pal(n = 9, name = "YlOrRd"), key = NULL, main = "Observed Transition Frequencies for 1st Proposed State Space", xlab = 'state s\'', ylab = 'state s')
plot(trans.prop.state.mc, digits=3, text.cell=list(cex=0.5), axis.col=list(side=1, las=1), axis.row = list(side=2, las=1), breaks = 9, col = RColorBrewer::brewer.pal(n = 9, name = "YlOrRd"), key = NULL, main = "Observed Transition Frequencies for 2nd Proposed State Space", xlab = 'state s\'', ylab = 'state s')

plot(reward.1(12), digits=3, text.cell=list(cex=0.5), axis.col=list(side=1, las=1), axis.row = list(side=2, las=1), breaks = 4, col = RColorBrewer::brewer.pal(n = 3, name = "Spectral"), key = NULL, main = "First Proposed Reward Function", xlab = 'state s\'', ylab = 'state s')

plot(reward.2(12), digits=3, text.cell=list(cex=0.5), axis.col=list(side=1, las=1), axis.row = list(side=2, las=1), breaks = 9, col = RColorBrewer::brewer.pal(n = 9, name = "Spectral"), key = NULL,xlab = 'state s\'', ylab = 'state s', main = "Second Proposed Reward Function")


```


```{r}
# Eigenvalue decomposition to find stationary dist. Only works with 'simple' state space
library(MASS)
# Get the eigenvectors of P, note: R returns right eigenvectors
r=eigen(trans.prop.state.simple)
rvec=r$vectors
# left eigenvectors are the inverse of the right eigenvectors
lvec=ginv(r$vectors)
# The eigenvalues
lam<-r$values
# Two ways of checking the spectral decomposition:
## Standard definition
rvec%*%diag(lam)%*%ginv(rvec)
rvec%*%diag(lam)%*%lvec
pi_eig<-lvec[1,]/sum(lvec[1,])
pi_eig
```





```{r}

# Function to create transition matrix of the data for each of actions 1, 2, and 3. n = number of states, pass colname & statecolname as strings. Works when states are in c(1, 2, 3)
actions.PTM <- function(colname, statecolname,  n, dataset = data.sorted){
  trans.counts1 <- matrix(data = 0, ncol = n, nrow = n)
  trans.counts2 <- matrix(data = 0, ncol = n, nrow = n)
  trans.counts3 <- matrix(data = 0, ncol = n, nrow = n)
for (i in 1:(dim(dataset)[1]-1)){
  if ((dataset[i,]$FK_transect_line == dataset[i+1,]$FK_transect_line) & (dataset[i,statecolname]==1)){
    trans.counts1[dataset[i,colname], dataset[i+1,colname]] = trans.counts1[dataset[i,colname], dataset[i+1,colname]] + 1
  }
  
  if ((dataset[i,]$FK_transect_line == dataset[i+1,]$FK_transect_line) & (dataset[i,statecolname]==2)){
    trans.counts2[dataset[i,colname], dataset[i+1,colname]] = trans.counts2[dataset[i,colname], dataset[i+1,colname]] + 1
    
  }
  if ((dataset[i,]$FK_transect_line == dataset[i+1,]$FK_transect_line) & (dataset[i,statecolname]==3)){
    trans.counts3[dataset[i,colname], dataset[i+1,colname]] = trans.counts3[dataset[i,colname], dataset[i+1,colname]] + 1
  }
  
}
  lst <- list(trans.counts1, trans.counts2, trans.counts3)
  return(lst)
}

```

```{r}
data.sorted$plant_community <- plyr::revalue(data.sorted$plant_community, c('Shrub'='1', 'Herb'='2'))
data.sorted$plant_community <- as.double(data.sorted$plant_community)
data.sorted$native_cover <- plyr::revalue(data.sorted$native_cover, c('N0'='1', 'N25'='2', 'N50'='3', 'N75' ='4'))
data.sorted$native_cover <- as.double(data.sorted$native_cover)
data.sorted$native_indicators <- plyr::revalue(data.sorted$native_indicators, c('I0'='1', 'I1'='2', 'I2'='3'))
data.sorted$native_indicators <- as.double(data.sorted$native_indicators)
```


```{r}
# using the function  above for each state
#state.actions <- actions.PTM('state', 'V10', 20, dataset = data.sorted)
#state.condensed.actions <- actions.PTM('state_condensed', 'V10', 12, dataset = data.sorted)
#state.mc.actions <- actions.PTM('state.mc', 'V10', 8, dataset = data.sorted)
#state.simple.actions <- actions.PTM('state.simple', 'V10', 4, dataset = data.sorted)
native.cover.jawn <- actions.PTM('native_cover', 'V13', 4, dataset = data.sorted)
native.indicators.jawn <- actions.PTM('native_indicators', 'V13', 3, dataset = data.sorted)
plant.community.jawn <- actions.PTM('plant_community', 'V13', 2, dataset = data.sorted)
```

```{r}
 # reward matrix where transitioning to a higher state is +1, staying the same is 0,  and lower is -1, size = # of states 
reward.1 <- function(size) {
  reward.mat1 <- matrix(data = 0, ncol = size, nrow = size)
  for (i in 1:size){
    for (j in 1:size){
     if (i<j){
       reward.mat1[i,j] <- 1
     }
      if (i>j){
      reward.mat1[i,j] <- -1
     }
  }}
return(reward.mat1)
    }

```

```{r}
 # reward matrix where transitioning to a higher or lower state is proportional to the difference in state number, size = # of states 

reward.2 <- function(size) {
  reward.mat1 <- matrix(data = 0, ncol = size, nrow = size)
  for (i in 1:size){
    for (j in 1:size){
  reward.mat1[i,j] <- j - i
     }
  }
return(reward.mat1)
}

```

```{r}
array.state <- array(c(state.actions[1], state.actions[2], state.actions[3]) , dim = c( 20 , 20 , 3 ))

array.state.condensed <- array(c(state.condensed.actions[[1]], state.condensed.actions[[2]], state.condensed.actions[[3]]), dim = c(12, 12, 3))

array.state.mc <- array(c(state.mc.actions[1], state.mc.actions[2], state.mc.actions[3]) , dim = c(8, 8, 3))

array.state.simple <- array(c(state.simple.actions[1], state.simple.actions[2], state.simple.actions[3]) , dim = c( 4 , 4 , 3 ))

array.plant.community <- array(c(plant.community.jawn[[1]], plant.community.jawn[[2]], plant.community.jawn[[3]]) , dim = c(2, 2, 3))
array.native.cover <- array(c(native.cover.jawn[[1]], native.cover.jawn[[2]], native.cover.jawn[[3]]) , dim = c(4, 4, 3))
array.native.indicator <- array(c(native.indicators.jawn[[1]], native.indicators.jawn[[2]], native.indicators.jawn[[3]]) , dim = c(3, 3, 3))

award1.12 <- reward.1(12) 
award.2 <- reward.1(2)
award.3 <- reward.1(3)
award.4 <- reward.1(4)
array.reward1.12 <- array(c(award1.12, award1.12, award1.12), dim = c( 12 , 12 , 3 ))
array.reward1.2 <- array(c(award.2, award.2, award.2), dim = c( 2 , 2 , 3 ))
array.reward1.3 <- array(c(award.3, award.3, award.3), dim = c( 3 , 3 , 3 ))
array.reward1.4 <- array(c(award.4, award.4, award.4), dim = c( 4 , 4 , 3 ))
```

```{r}
library(MDPtoolbox)
# https://cran.r-project.org/web/packages/MDPtoolbox/MDPtoolbox.pdf 
# there are several algorithms to choose from & compare against, for example: 
mdp1 <- mdp_Q_learning(P = array.state.condensed, R = array.reward1.12, discount = .9)
mdp2 <- mdp_LP(P = array.state.condensed, R = array.reward1.12, discount = .9)
mdp3 <- mdp_finite_horizon(P = array.state.condensed, R = array.reward1.12, discount = .9, N = 3)
```

```{r}
sequ <- seq(.5, 1, by = .05)
policy_check <- data.frame(matrix(NA, nrow = length(sequ), ncol = 12))
native.ind.pol <- data.frame(matrix(NA, nrow = length(sequ), ncol = 3))
native.cover.pol <- data.frame(matrix(NA, nrow = length(sequ), ncol = 4))
plant.comm.pol <- data.frame(matrix(NA, nrow = length(sequ), ncol = 2))
for (i in 1:length(sequ)){
  mdp1 <- mdp_Q_learning(P = array.state.condensed, R = array.reward1.12, discount = sequ[i])
  mdp2 <- mdp_LP(P = array.state.condensed, R = array.reward1.12, discount = sequ[i])
  mdp3 <- mdp_finite_horizon(P = array.state.condensed, R = array.reward1.12, discount = sequ[i], N = 3)
  policy_check[i,] <- t(mdp1$policy)
  policy_check[i + 11,] <- t(mdp2$policy)
  policy_check[i + 22,] <- t(mdp3$policy)
  mdp4 <- mdp_Q_learning(P = array.native.indicator, R = array.reward1.3, discount = sequ[i])
  mdp5 <- mdp_LP(P = array.native.indicator, R = array.reward1.3, discount = sequ[i])
  mdp6 <- mdp_finite_horizon(P = array.native.indicator, R = array.reward1.3, discount = sequ[i], N = 3)
  native.ind.pol[i,] <- t(mdp4$policy)
  native.ind.pol[i + 11,] <- t(mdp5$policy)
  native.ind.pol[i + 22,] <- t(mdp6$policy)
    mdp7 <- mdp_Q_learning(P = array.native.cover, R = array.reward1.4, discount = sequ[i])
  mdp8 <- mdp_LP(P = array.native.cover, R = array.reward1.4, discount = sequ[i])
  mdp9 <- mdp_finite_horizon(P = array.native.cover, R = array.reward1.4, discount = sequ[i], N = 3)
  native.cover.pol[i,] <- t(mdp7$policy)
  native.cover.pol[i + 11,] <- t(mdp8$policy)
  native.cover.pol[i + 22,] <- t(mdp9$policy)
    mdp11 <- mdp_Q_learning(P = array.plant.community, R = array.reward1.2, discount = sequ[i])
  mdp22 <- mdp_LP(P = array.plant.community, R = array.reward1.2, discount = sequ[i])
  mdp33 <- mdp_finite_horizon(P = array.plant.community, R = array.reward1.2, discount = sequ[i], N = 3)
  plant.comm.pol[i,] <- t(mdp11$policy)
  plant.comm.pol[i + 11,] <- t(mdp22$policy)
  plant.comm.pol[i + 22,] <- t(mdp33$policy)

}
```

```{r}
barplots <- data.frame(matrix(0, nrow = 3, ncol = 12))
barplots.natcov <- data.frame(matrix(0, nrow = 3, ncol = 4))
barplots.plantcom <- data.frame(matrix(0, nrow = 3, ncol = 2))
barplots.natind <- data.frame(matrix(0, nrow = 3, ncol = 3))
for (i in 1:12){
  for (j in 1:33){
  if (policy_check[j,i] == 1){
    barplots[1,i] <- barplots[1,i] +1
  }
  else if (policy_check[j,i] == 2){
    barplots[2,i] <- barplots[2,i] +1
  }
  else if (policy_check[j,i] == 3){
    barplots[3,i] <- barplots[3,i] +1
  }}
  
}
```

```{r}
for (i in 1:3){
  for (j in 1:33){
  if (native.ind.pol[j,i] == 1){
    barplots.natind[1,i] <- barplots.natind[1,i] +1
  }
  else if (native.ind.pol[j,i] == 2){
    barplots.natind[2,i] <- barplots.natind[2,i] +1
  }
  else if (native.ind.pol[j,i] == 3){
    barplots.natind[3,i] <- barplots.natind[3,i] +1
  }}
  
}
for (i in 1:4){
  for (j in 1:33){
  if (native.cover.pol[j,i] == 1){
    barplots.natcov[1,i] <- barplots.natcov[1,i] +1
  }
  else if (native.cover.pol[j,i] == 2){
    barplots.natcov[2,i] <- barplots.natcov[2,i] +1
  }
  else if (native.cover.pol[j,i] == 3){
    barplots.natcov[3,i] <- barplots.natcov[3,i] +1
  }}
  
}
for (i in 1:2){
  for (j in 1:33){
  if (plant.comm.pol[j,i] == 1){
    barplots.plantcom[1,i] <- barplots.plantcom[1,i] +1
  }
  else if (plant.comm.pol[j,i] == 2){
    barplots.plantcom[2,i] <- barplots.plantcom[2,i] +1
  }
  else if (plant.comm.pol[j,i] == 3){
    barplots.plantcom[3,i] <- barplots.plantcom[3,i] +1
  }}
  
}
```


```{r}

pointy <- c(.05)
barplots <- as.matrix(barplots)
colnames(barplots) <- c('1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12')
barplot(height = barplots, 
        col=c("lightblue1","palegreen1", 'salmon'), xlab = 'State', ylab = 'Number of Policies', main = 'Density of Aggregated \'Optimal\' Policies by State')
legend(title = 'Actions', "topright", c("Leave","Burn", "Graze"), pch=15, 
       col=c("lightblue1","palegreen1", 'salmon'), 
       bty="n", xpd = T, inset=c(-0.08, 0), cex = .9)
#title(ylab="Density of Aggregated Policies", line=0, cex.lab=1)

```

```{r}
barplots.natind <- as.matrix(barplots.natind)
colnames(barplots.natind) <- c('<0.1', '0.1 - 0.2', '>0.2')
barplot(height = barplots.natind, 
        col=c("lightblue1","palegreen1", 'salmon'), xlab = 'State', ylab = 'Number of Policies', main = 'Density of Aggregated \'Optimal\' Policies for Native Indicators')
legend(title = 'Actions', "topright", c("Leave","Burn", "Graze"), pch=15, 
       col=c("lightblue1","palegreen1", 'salmon'), 
       bty="n", xpd = T, inset=c(-0.08, 0), cex = .9)
```


```{r}
barplots.plantcom <- as.matrix(barplots.plantcom)
colnames(barplots.plantcom) <- c('Shrub', 'Herb')
barplot(height = barplots.plantcom, 
        col=c("lightblue1","palegreen1", 'salmon'), xlab = 'State', ylab = 'Number of Policies', main = 'Density of Aggregated \'Optimal\' Policies for Plant Community')
legend(title = 'Actions', "topright", c("Leave","Burn", "Graze"), pch=15, 
       col=c("lightblue1","palegreen1", 'salmon'), 
       bty="n", xpd = T, inset=c(-0.08, 0), cex = .9)
```


```{r}
barplots.natcov <- as.matrix(barplots.natcov)
colnames(barplots.natcov) <- c('0-25', '25-50', '50-75', '75+')
barplot(height = barplots.natcov, 
        col=c("lightblue1","palegreen1", 'salmon'), xlab = 'State', ylab = 'Number of Policies', main = 'Density of Aggregated \'Optimal\' Policies for Native Cover')
legend(title = 'Actions', "topright", c("Leave","Burn", "Graze"), pch=15, 
       col=c("lightblue1","palegreen1", 'salmon'), 
       bty="n", xpd = T, inset=c(-0.08, 0), cex = .9)
```




Methods

Our first task was to create a simplified state space that would make the probability transition matrix less sparse. There is not much literature about defining state spaces for state transition models, so we propose 3 alternative state-spaces using a similar method that the Grassland Monitoring Team used. The first simplified state-space shrinks the number of states to 12 by combining some of the states with very low overall counts. The second simplified state space takes shrinks the number of states to 12 by trying to remove all instances of where the probability of transitioning from one state to another is 0. The final proposed state space is an oversimplified state space that reduces our number of states to four. This state space is less useful for creating a state transition model, but could be used in the future for a markovc chain analysis which could evaluate their current methodology of choosing actions to perform. Figure 5 shows how each state space was chosen based on the collected data. 

To create a transition model we decided that we would use a Markov Decision Process. 'a Markov Decision Process (MDP) is a mathematical framework used for modeling decision-making problems where the outcomes are partly random and partly controllable. ' (4). 

The use of a MDP relies on a few important assumptions. First, the actions in a MDP need to be completely controlled and the states must be completely observable. In our case, the first assumption definitely holds, however the second may not. Sampling prairies is expensive in terms of time and manpower, so many of the transects in the dataset were sampled every 2 or 3 years. Also, once the data for decisions made is aggregated, we expect that many of the decisions of leaving/burning/grazing the prairies were made on off years when the prairies were not measured. There are workarounds to this which will be discussed later, but for now we assume that both of these assumptions hold. 
Finally, the states in the system need to follow the Markov Property. The Markov Property states that the future of a state depends only on the present and not the past. In our case, the current state of the prairie transect contain all of the necesary information of past states. Mathematically speaking, the probability of moving to a state $S_{t+1}$ from state $S_t$ is only dependant on state $S_t$. We agreed that this fit the process of the prairie ecosystem. 

How does a MDP work?

To solve a MDP we need 5 things:

\item $P$, a probability transition matrix. We determined our probability transition matrix by bootstrap sampling the observed probability transition matrix from the data. 

\item $S$, a set of states.  These are the state spaces of the prairie transects

\item $A$, a set of actions. These are Leave, Burn, and Graze. 

\item $R$, a reward function, which assigns some value given for reaching state $S_{t+1}=s'$ from state $S_{t} = s$

\item $\gamma$, a discount for future awards. 

The goal of a MDP is to come up with a policy $\pi(s)$ which will determine the optimal action $a$ to take at state $s$.

Choosing the best possible policy depends upon finding the optimal state-value function and the optimal action value function. To show this functions, we first need to define the peices. 


When The Grassland Monitoring team takes an action on a prairie under a policy $\pi(s)$, the transition probability matrix $\textbf P$ determines the subsequent state $s$.

$\textbf P^a_{ss'} = P(S_{t+1} = s' | S_t=s, A_t =a)$

After a transect transitions from state $s$ to $s'$ after a policy $\pi(a|s)$ was followed, it gets a reward based on the reward function $\textbf R$ as feedback.

$\textbf R^a_{s} = E[R_{t+1} | S_t = s, A_t = a]$

This is a short term reward that is recieved after transitioning from state $s$ to state $s'$ given some action $a$. Summing all future rewards and discounting them with the discount factor $\gamma$ gives us our return $\textbf G$. $\gamma$ is a value that ranges from 0 to 1. The closer to 1 $\gamma$ is, the more weight is put on future rewards. Choosing $\gamma$ lets you balance between short and long term rewards.(6) 

$\textbf G_t = R_{t+1} + \gamma R_{t+2}+ \gamma^2 R_{t+3} +...\gamma^k R_{t+k+1} = \Sigma^N_{i=0} \gamma^k R_{t+i+1}$

With the return $\textbf G_t$ defined, we can define the state-value and action-value functions.

Following some policy $\pi$, the state value function $V_{\pi}(s)$ tells us how good it is to be in state $s$. In a similar vein, the action-value function $Q_{\pi}(s)$ tells how good it is to take that action. 

$V_{\pi}(s) = E_{\pi}[\textbf G_t | S_t=s]$

$Q_{\pi}(s, a) = E_{\pi}[\textbf G_t | S_t=s, A_t=a]$


Optimal Policy

An optimal policy, $\pi_*$, would give us the optimal state-value and action-value equations which in turn maximizes the return $\textbf G_t$.

$\pi_* = argmax_{\pi}V_{\pi}(s) = argmax_{\pi}Q_{\pi}(s,a)$

To calculate these argmax's, we rely on the Bellmen equations. The Bellman equation breaks the state-value and action-value functions into their immediate reward and the future value function.

$V_{\pi}(s) = E_{\pi}[\textbf G_t | S_t=s]$
$= E_{\pi}[R_{t+1} + \gamma (R_{t+2}+ \gamma R_{t+3} +...) | S_t=s]$
$= E_{\pi}[R_{t+1} + \gamma \textbf G_{t+1}| S_t=s]$
$= E_{\pi}[R_{t+1} + \gamma V_{\pi}(s+1)| S_t=s]$

$Q_{\pi}(s,a) = E_{\pi}[\textbf G_t | S_t=s, A_t = a]$
$= E_{\pi}[R_{t+1} + \gamma (R_{t+2}+ \gamma R_{t+3} +...) | S_t=s, A_t = a]$
$= E_{\pi}[R_{t+1} + \gamma \textbf G_{t+1}| S_t=s, A_t = a]$
$= E_{\pi}[R_{t+1} + \gamma V_{\pi}(s+1, a+1)| S_t=s, A_t = a]$


